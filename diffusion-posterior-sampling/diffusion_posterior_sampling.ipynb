{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementierung von Diffusion Posterior Sampling (DPS)\n",
    "\n",
    "Hier werden wir selbst die Methode \"Diffusion Posterior Sampling\" (DPS) von [Chung et al., 2022](https://arxiv.org/abs/2209.14687) implementieren.\n",
    "DPS ist eine Methode zum Lösen linearer und nichtlinearer inverser Probleme unter Nutzung eines vortrainierten Diffusion-Models als Prior.\n",
    "\n",
    "Wir nutzen an einigen Stellen das [DeepInverse (`deepinv`) Python-Paket](https://deepinv.github.io/deepinv/), welches für das Lösen inverser Probleme designed wurde, damit wir nicht jede einzelne Operation von null auf selbst implementieren müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-Setup\n",
    "Wir definieren uns hier ein 'random inpainting' problem, bei dem zufällig (aber fix) gewählte Pixel im Bild fehlen und aus den verbleibenden Pixeln rekonstruiert werden müssen.\n",
    "\n",
    "Wir laden dazu als erstes ein Beispielbild von Größe 64 x 64, welches wir als unsere \"ground truth\" (wahres Bild) nutzen. Wir repräsentieren das Bild in diesem Kontext immer als ein Array von Gleitkommazahlen (float32), dessen Werte zwischen -1.0 und 1.0 liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt, matplotlib as mpl\n",
    "import ipywidgets as widgets\n",
    "%matplotlib widget\n",
    "\n",
    "import deepinv as dinv\n",
    "from deepinv.utils.plotting import plot\n",
    "from deepinv.optim.data_fidelity import L2\n",
    "from deepinv.utils.demo import load_image\n",
    "from tqdm import tqdm  # to visualize progress\n",
    "\n",
    "def todo():\n",
    "    raise NotImplementedError(\"In dieser Zelle gibt es noch mindestens ein TODO!\")\n",
    "def check_for_nan(var, var_name):\n",
    "    if var.isnan().any():\n",
    "        raise ValueError(f\"NaN (not-a-number) Wert wurde in der Variable {var_name} gefunden! Abbruch...\")\n",
    "\n",
    "device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "x_true = load_image(\"example_images/butterfly.png\", img_size=64).to(device) * 2 - 1  # map from deepinv's loading range [0, 1] to [-1, 1]\n",
    "print(\"Type of x_true:\", x_true.dtype, \"; min =\", x_true.min(), \"; max =\", x_true.max())\n",
    "x = x_true.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann definieren wir uns das 'random inpainting' Problem und den zugehörigen Vorwärts-Operator, der hier `physics` genannt wird.\n",
    "Wir simulieren hier, dass 90% der Pixel im Bild fehlen, und aus den verbleibenden 10% der Pixel rekonstruiert werden müssen.\n",
    "Außerdem verrauschen wir die verbleibenden Pixel mit Gaussian Noise von Varianz $\\sigma^2 = 0.01^2$, was die Schwierigkeit des Problems nochmal etwas erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sigma = 0.01  # noise level\n",
    "\n",
    "physics = dinv.physics.Inpainting(\n",
    "    img_size=(3, x.shape[-2], x.shape[-1]),\n",
    "    mask=0.1,  # wie viele Pixel (1.0 = 100%, 0.1 = 10%) sind in der Messung y übrig?\n",
    "    pixelwise=True,\n",
    "    noise_model=dinv.physics.GaussianNoise(sigma=sigma),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "y = physics(x_true)\n",
    "\n",
    "imgs = [y, x_true]\n",
    "plot(\n",
    "    imgs,\n",
    "    titles=[\"measurement\", \"ground-truth\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden eines vortrainierten Diffusion Models\n",
    "\n",
    "Da wir nicht genügend Ressourcen und Zeit haben, hier unser eigenes echtes Diffusion-Model auf Bildern zu trainieren, nutzen wir ein vortrainiertes Model (deep neural net / DNN).\n",
    "\n",
    "Dieses Model wurde auf dem [FFHQ dataset](https://github.com/NVlabs/ffhq-dataset) in Auflösung 256x256 trainiert, welches nur Bilder von menschlichen Gesichtern enthält. Das ist zwar kein besonders guter Prior für unser Beispielbild von einem Schmetterlingsflügel, funktioniert aber für das gewählte inverse Problem trotzdem visuell ganz gut. Der Grund für die Nutzung des Modells ist hier hauptsächlich, dass es kein so riesiges DNN ist und daher nicht so lange braucht um ausgewertet zu werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dnn = dinv.models.DiffUNet(large_model=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definieren der Diffusion-Schedule\n",
    "\n",
    "Wir nutzen hier die Standard-Schedule aus [DDPM (Ho et al., 2020)](https://arxiv.org/pdf/2006.11239). Dort wird zunächst $\\beta_t$ als lineare schedule zwischen gewählten Konstanten $\\beta_{\\rm min}$ und $\\beta_{\\rm max}$ definiert. Dann werden die daraus folgenden Koeffizienten definiert:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_t := 1 - \\beta_t\\\\\n",
    "\\bar\\alpha_t := \\prod_{j=1}^t \\alpha_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hier nochmal wiederholt die Formeln für den Vorwärts-Diffusionsprozess vom Freitag:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{x}_t | \\mathbf{x}_{t-1}, \\varepsilon &= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0, I)  \\qquad (\\text{schrittweise})\\\\\n",
    "\\mathbf{x}_t | \\mathbf x_0, \\varepsilon' &= \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_t}\\mathbf \\varepsilon',\\qquad \\varepsilon' \\sim \\mathcal{N}(0, I) \\qquad (\\text{direkt von}\\ \\mathbf x_0)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Trainiert wurde das DNN mit 1000 Schritten, was ihr auch an `betas.shape` sehen könnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# These are the parameters that were used for the noise schedule during model training\n",
    "\n",
    "num_train_timesteps = 1000  # Number of timesteps used during training\n",
    "beta_min = 0.1 / 1000\n",
    "beta_max = 20 / 1000\n",
    "betas = torch.linspace(beta_min, beta_max, num_train_timesteps).to(device)\n",
    "alphas = (1 - betas)\n",
    "alpha_bars = alphas.cumprod(dim=0)\n",
    "\n",
    "betas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition eines Denoisers basierend auf dem Diffusion-Model\n",
    "\n",
    "Da das Diffusion-Model effektiv dafür trainiert wurde, den Noise-Anteil $z$ in einem Bild $\\mathbf x_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\mathbf z$ zu schätzen, können wir es auch als Denoiser von $\\mathbf x_t$ einsetzen. Gegeben $\\mathbf x_t$ und $t$ gibt uns das Diffusion-Model $\\hat{\\varepsilon} = D_\\theta(\\mathbf x_t, t)$ als eine Schätzung von $\\varepsilon$. Wir können die Gleichung einfach umstellen und bekommen:\n",
    "\n",
    "$$\\hat{\\mathbf x}_0 = \\frac{\\mathbf x_t - (1 - \\sqrt{\\bar{\\alpha}_t}) \\hat{\\mathbf \\varepsilon}}{\\sqrt{\\bar{\\alpha}_t}}$$\n",
    "\n",
    "\n",
    "**Aufgabe:**\n",
    "* Implementiert $\\hat{\\mathbf x}_0$ in der `denoiser(xt, t)` Funktion. Nutzt die dort vordefinierte Variable `noise_estimate`, die das geschätzte $\\hat{\\varepsilon}$ enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_noise_estimate(xt, t):\n",
    "    \"\"\"A little wrapper around the DNN forward() call to return only the estimated noise\"\"\"\n",
    "    noise_est_sample_var = dnn(xt, t, type_t=\"timestep\")\n",
    "    noise_est = noise_est_sample_var[:, :3, ...]\n",
    "    return noise_est\n",
    "\n",
    "\n",
    "def denoiser(xt, t):\n",
    "    if isinstance(t, int):\n",
    "        # convert integer timesteps to timestep tensors\n",
    "        t = torch.tensor([t]*xt.shape[0], device=xt.device, dtype=torch.long)\n",
    "    # this is an estimate of the zero-mean unit-variance added Gaussian noise sample epsilon ~ N(0, I)\n",
    "    epshat = get_dnn_noise_estimate(xt, t)\n",
    "\n",
    "    ### PARTICIPANT TODO\n",
    "\n",
    "    # x0_hat = todo()\n",
    "\n",
    "    ### PARTICIPANT TODO END\n",
    "\n",
    "    return x0_hat, epshat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t = 200  # as an example for single-step denoising: choose some arbitrary timestep in [0, 999]\n",
    "\n",
    "# Draw x_t sample based on ground-truth x_0\n",
    "x0 = x_true\n",
    "xt = alpha_bars[t].sqrt() * x0 + (1 - alpha_bars[t]).sqrt() * torch.randn_like(x0)\n",
    "\n",
    "# Apply the denoiser \n",
    "x0_t, epshat_t = denoiser(xt, t)\n",
    "\n",
    "# Visualize\n",
    "imgs = [x0, xt, x0_t, epshat_t]\n",
    "plot(\n",
    "    imgs,\n",
    "    titles=[\"ground-truth\", \"noisy\", r\"denoised $\\widehat{\\mathbf x}_0$\", r\"noise estimate $\\hat{\\varepsilon}$\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der DPS-Algorithmus\n",
    "\n",
    "Nachdem wir jetzt ein Vorwärts-Modell `physics` und einen diffusion-basierten Denoiser `denoiser(xt, t)` definiert haben, können wir den DPS-Algorithmus implementieren.\n",
    "\n",
    "DPS kombiniert in einem Bayes-inspirierten Sinne einen Gradienten bezüglich der *likelihood*, gegeben durch das Vorwärts-Modell, mit einem Gradienten bezüglich des *priors*, gegeben durch das Diffusion Model.\n",
    "\n",
    "## Wiederholung von der Tafel:\n",
    "\n",
    "Um gradienten-basiertes **posterior sampling** mit einem Diffusion-Model zu realisieren, möchten wir den folgenden Term ausrechnen:\n",
    "$$\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y})$$\n",
    "\n",
    "Durch den Satz von Bayes, die Eigenschaften des Logarithmus, und des Gradienten gelangen wir zu:\n",
    "\n",
    "\\begin{align*}\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y}) = \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "          + \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\mathbf{x}_t)\\end{align*}\n",
    "\n",
    "**Frage:** Wo ist der Term $p(y)$ aus dem Satz von Bayes hin...?\n",
    "\n",
    "Für den ersten der beiden Terme können wir unser Vorwärts-Modell nutzen, indem wir durch dieses durch differenzieren. Die Aufgabe übernimmt hier `deepinv` bzw. PyTorch für uns.\n",
    "Der zweite Term steht uns auch nach dem Training eines Diffusion-Models leider nicht zur Verfügung, aus theoretischen Gründen – welche wir, wenn ihr das hier lest, bereits besprochen haben sollten :).\n",
    "DPS schlägt zur Approximation dieses Terms Folgendes vor (siehe Theorem 1 aus [Chung et al., 2022](https://arxiv.org/abs/2209.14687) für Details):\n",
    "\n",
    "\\begin{align*}\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t|\\mathbf{y}) \\approx \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "          + \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{y}|\\widehat{\\mathbf{x}}_{0}(\\mathbf{x_t}, t))\\end{align*}\n",
    "\n",
    "wobei wir zum Ausrechnen von $\\widehat{\\mathbf x}_0(\\mathbf x_t, t)$ genau unseren oben definierten Denoiser nutzen können! :)\n",
    "Unter der Modell-Annahme, dass $y$ Gauss-verteilt ist (siehe unser noise model oben), gilt für die log-probability im zweiten Term:\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\\log p(\\mathbf{y}|\\widehat{\\mathbf{x}}_0(\\mathbf{x_t})) =\n",
    "      -\\frac{\\|\\mathbf{y} - A\\widehat{\\mathbf{x}}_0(\\mathbf{x_t}, t)\\|_2^2}{2\\sigma_y^2}.\\end{align*}\n",
    "\n",
    "und um den zugehörigen Gradienten zu bekommen, nutzen wir die *automatic differentiation* Fähigkeiten von PyTorch (`torch.autograd`).\n",
    "\n",
    "**Aufgabe**: In der nächsten Zelle testen wir das für ein einzelnes $t = 150$. Führt die nächste Zelle aus und versucht den kommentierten Abschnitt \"DPS likelihood gradient\" nachzuvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x0 = x_true\n",
    "\n",
    "get_likelihood_loss = L2()  # L2-Loss für den likelihood loss\n",
    "\n",
    "# Sample xt ~ q(xt|x0)\n",
    "t = 150  # choose some arbitrary timestep\n",
    "at = alpha_bars[t]\n",
    "sigma_cur = (1 - at).sqrt()\n",
    "xt = at.sqrt()*x0 + sigma_cur * torch.randn_like(x0)\n",
    "\n",
    "# DPS likelihood gradient\n",
    "with torch.enable_grad():\n",
    "    # Turn on gradients for xt so we can use autograd\n",
    "    xt.requires_grad_()\n",
    "    # Call the denoiser, get estimates for clean image x0_t and noise epshat_t\n",
    "    x0_t, epshat_t = denoiser(xt, t)\n",
    "    # Log-likelihood\n",
    "    likelihood_loss = get_likelihood_loss(x0_t, y, physics).sum()\n",
    "    # Take gradient w.r.t. xt\n",
    "    likelihood_grad = torch.autograd.grad(outputs=likelihood_loss, inputs=xt)[0]\n",
    "\n",
    "# Visualize\n",
    "imgs = [x0, xt, x0_t, epshat_t, likelihood_grad]\n",
    "plot(\n",
    "    imgs,\n",
    "    titles=[\"ground-truth\", f\"noisy ({t=})\", r\"denoised $\\hat{\\mathbf x}_0$\", r\"noise estimate $\\hat{\\varepsilon}$\", \"likelihood gradient\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein Screenshot von Pseudocode des Algorithmus aus dem [DPS-Paper](https://arxiv.org/pdf/2209.14687), siehe dort \"Algorithm 1\":\n",
    "\n",
    "![image.png](dps_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe**: Implementiert den DPS-Algorithmus in Python.\n",
    "<br>\n",
    "**Hinweise:**\n",
    "* Der Index $i$ meint hier immer unsere Diffusion Time $t$, und wir nutzen im Code immer $t$ als Index.\n",
    "* $\\mathbf z$ meint hier unser $\\varepsilon$, also das noise sample.\n",
    "* Ein paar Größen sind für euch vordefiniert, damit ihr nicht alles nachschlagen müsst, insbesondere $\\tilde{\\sigma}_i$ (`sigma_t`) und $\\nabla_{x_i} \\lVert \\mathbf y - \\mathcal{A}(\\hat{\\mathbf x}_0) \\rVert_2$ (`likelihood_grad`), sowie $\\alpha_i, \\bar{\\alpha}_i$, $\\bar{\\alpha}_{i-1}$, $\\zeta_i$ (`a_t`, `abar_t`, `abar_t_next`, `zeta_t`).\n",
    "* Da ein Durchlauf mit 1000 Iterationen leider 30 Sekunden bis zu ein paar Minuten läuft, ist es zu empfehlen, eure Lösung etwas genauer anzuschauen, bevor ihr sie nach jeder Änderung sofort ausprobiert (:\n",
    "<!-- * Implementiert `zeta_t` = $\\zeta_i := \\zeta / \\big\\lVert y - A(\\widehat{\\mathbf x}_0) \\big\\rVert_2$, wobei `zeta` = $\\zeta$ ein frei gewählter konstanter Hyperparameter ist.\n",
    "    * Hier ist `torch.linalg.vector_norm` mit `dim=(-2, -1)` eine hilfreiche Funktion.\n",
    "    * Wir haben schon mal `zeta = 0.5` vordefiniert, womit unser inverses Problem hier ganz gut klappen sollte, wenn eure Implementation stimmt.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm hyperparameters:\n",
    "# * zeta: the likelihood gradient scale/strength. usually the sweet spot is somewhere between 0.1 and 3.0\n",
    "zeta = 1.0\n",
    "\n",
    "# simulate measurement\n",
    "x0 = x_true\n",
    "y = physics(x0.to(device))\n",
    "\n",
    "# sample an initial image estimate: just a sample from x_T, where p(x_T) = N(0, I)\n",
    "x = torch.randn_like(x0)\n",
    "xs = [x]\n",
    "x0_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in tqdm(reversed(range(0, num_train_timesteps)), total=num_train_timesteps):\n",
    "        a_t = alphas[t]\n",
    "        abar_t = alpha_bars[t]\n",
    "        abar_t_next = alpha_bars[t - 1] if t - 1 >= 0 else torch.tensor(1)\n",
    "        b_t = betas[t]\n",
    "        \n",
    "        sigma_tilde = torch.sqrt(b_t * (1 - abar_t_next) / (1 - abar_t))  # see Eq. 7 in DDPM paper\n",
    "\n",
    "        # Get the denoised image x0_hat and its likelihood gradient\n",
    "        xt = xs[-1].to(device)\n",
    "        with torch.enable_grad():\n",
    "            xt.requires_grad_()\n",
    "            # 1. denoising step\n",
    "            x0_hat, eps_hat = denoiser(xt, t)\n",
    "            # 2. likelihood gradient approximation\n",
    "            likelihood_loss = get_likelihood_loss(x0_hat, y, physics).sum()\n",
    "        likelihood_grad = torch.autograd.grad(outputs=likelihood_loss, inputs=xt)[0].detach()\n",
    "        zeta_t = zeta / torch.linalg.vector_norm(likelihood_grad, ord=2, dim=(-2, -1), keepdim=True)\n",
    "        eps = torch.randn_like(xt)  # new noise sample for this iteration, different from eps_hat\n",
    "\n",
    "        ### PARTICIPANT TODO\n",
    "\n",
    "        # xt_next_prime = todo()\n",
    "        # zeta_t = todo()\n",
    "        # xt_next = xt_next_prime - todo()\n",
    "\n",
    "        ### PARTICIPANT TODO END\n",
    "\n",
    "        check_for_nan(x0_hat, \"x0_hat\")\n",
    "        check_for_nan(xt_next, \"xt_next\")\n",
    "        x0_preds.append(x0_hat.to(\"cpu\"))\n",
    "        xs.append(xt_next.to(\"cpu\"))\n",
    "\n",
    "recon = xs[-1]\n",
    "\n",
    "# plot the results\n",
    "imgs = [y, recon, x_true]\n",
    "plot(imgs, titles=[\"measurement\", \"model output\", \"ground-truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = num_train_timesteps\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(7, 2.5))\n",
    "shown_measurement = y.detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1)\n",
    "shown_measurement[shown_measurement.isclose(torch.tensor(0.0))] = np.nan  # hide masked-out pixels (zeros)\n",
    "im0 = axs[0].imshow((shown_measurement + 1) / 2)\n",
    "im1 = axs[1].imshow((xs[-1].detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)\n",
    "im2 = axs[2].imshow((x_true.detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)\n",
    "axs[0].set_title('Measurement')\n",
    "axs[1].set_title('Model output')\n",
    "axs[2].set_title('Ground truth')\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "fig.tight_layout()\n",
    "\n",
    "@widgets.interact(t=(0, num_steps-1, 1))\n",
    "def update(t=0):\n",
    "    im1.set_data((xs[num_steps - t - 1].detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus 1: Mit anderen Bildern rumspielen\n",
    "\n",
    "Ladet ganz oben statt `example_images/butterfly.png` andere Bilder aus dem Ordner `example_images`. Setzt dabei noch die Option `resize_mode='resize'` in der `load_image` Funktion. Falls ihr eine GPU habt, könnt ihr auch versuchen den `img_size` Parameter wegzunehmen, sodass die volle Auflösung (256 x 256) genutzt wird. Probiert dann z.B. verschiedene `zeta` Einstellungen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus 2: Eine einfachere Formulierung mit der Möglichkeit, Zeitschritte zu überspringen\n",
    "\n",
    "(Ab hier nur Bonus für Leute die noch Zeit und Lust haben!)\n",
    "\n",
    "Es ist ein bisschen sehr kostspielig und langwierig, das neuronale Netz 1000 mal auszuwerten. Wir können hier aber durch ein paar geschickte Ideen Zeitschritte überspringen und so nur bspw. 200 Auswertungen erreichen.\n",
    "\n",
    "Zeile 6 des DPS-Algorithmus definiert $\\mathbf x_{t-1}'$ auf Basis der Größen\n",
    "$$\\{ \\mathbf x_t, \\hat{\\mathbf x}_0, \\mathbf \\varepsilon_{\\text{next}} \\}$$\n",
    "wobei wir hier $\\mathbf \\varepsilon_{\\text{next}}$ für den neu gesampelten Gaussian Noise schreiben, um ihn klarer von dem aus dem vorherigen Bild geschätzten Noise zu unterscheiden. Da $\\mathbf x_t$ aber einfach eine gewichtete Summe aus $\\hat{\\mathbf x}_0$ und der Schätzung des vorherigen Noise $\\hat{\\mathbf \\varepsilon}$ ist, kann man $\\mathbf x_{t-1}'$ aber stattdessen auch auf Basis der Größen\n",
    "$$\\{ \\hat{\\mathbf x}_0, \\hat{\\mathbf \\varepsilon}, \\mathbf \\varepsilon_{\\text{next}} \\}$$ schreiben.\n",
    "\n",
    "**Aufgabe**: Leitet euch durch Umstellen den expliziten Ausdruck für $\\mathbf x_t$ als Funktion von $\\hat{\\mathbf x}_0$ und $\\hat{\\mathbf \\varepsilon}$ her und schreibt ihn euch auf.\n",
    "\n",
    "Man kann dann algebraisch mit einigem Aufwand zeigen, dass Zeile 6 aus DPS äquivalent ist zu:\n",
    "$$\n",
    "\\mathbf x_{t-1}' = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{\\mathbf x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\tilde{\\sigma}_t^2} \\hat{\\mathbf \\varepsilon} + \\tilde{\\sigma}_t \\mathbf \\varepsilon_{\\text{next}}\n",
    "$$\n",
    "\n",
    "**Frage**: Was sind Vorteile dieses Ausdrucks gegenüber Zeile 6 aus DPS, wenn man darüber nachdenkt, Zeitschritte zu überspringen?\n",
    "\n",
    "**Antwort**:\n",
    "Der Ausdruck hängt nicht mehr explizit von Konstanten zum aktuellen Zeitschritt $t$ ab, sondern nur von Konstanten zum nächsten Zeitschritt $t-1$. Wir können damit nun auch Zeitschritte überspringen, indem wir statt $t-1$ immer $t-s$ schreiben, für einen \"Skip\"-Parameter $s$. Das erlaubt uns schnelleres Sampling, da wir die Schleife bspw. nur 200 statt 1000 mal durchlaufen müssen.\n",
    "\n",
    "**Aufgabe**:\n",
    "* Implementiert den neuen Ausdruck für `xt_next_prime` im Code hierunter.\n",
    "* Kopiert euren Ausdruck für `xt_next` auf Basis von `xt_next_prime` von eurer ersten DPS-Implementierung.\n",
    "* Führt dann die Zelle aus, probiert verschiedene `num_steps` und `zeta` aus und diskutiert eure Beobachtungen.\n",
    "\n",
    "**Fragen**:\n",
    "* Warum können wir hier nicht ein völlig beliebiges `num_steps` setzen, z.B. 172?\n",
    "* Warum müssen wir evtl. `zeta` anpassen, wenn wir `num_steps` ändern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm hyperparameters:\n",
    "# * zeta: the likelihood gradient scale/strength. usually the sweet spot is somewhere between 0.1 and 3.0\n",
    "# * num_steps: number of steps, used for skipping\n",
    "zeta = 1.0  # set to a reasonable default for num_steps = 100\n",
    "num_steps = 200\n",
    "assert num_steps <= 1000, \"more than 1000 steps not possible with this model\"\n",
    "skip = num_train_timesteps // num_steps  # how many steps are we skipping each iteration due to num_steps < 1000?\n",
    "\n",
    "# simulate measurement\n",
    "x0 = x_true\n",
    "y = physics(x0.to(device))\n",
    "\n",
    "# sample an initial image estimate: just a sample from x_T, where p(x_T) = N(0, I)\n",
    "x = torch.randn_like(x0)\n",
    "xs_fast = [x]\n",
    "x0_preds_fast = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # loop step skipping happens here (range with step size \"skip\")\n",
    "    for t in tqdm(reversed(range(0, num_train_timesteps, skip)), total=num_steps):\n",
    "        a_t = alphas[t]\n",
    "        b_t = betas[t]\n",
    "        abar_t = alpha_bars[t]\n",
    "        abar_t_next = alpha_bars[t - skip] if t - skip >= 0 else torch.tensor(1)\n",
    "\n",
    "        xt = xs_fast[-1].to(device)\n",
    "        with torch.enable_grad():\n",
    "            xt.requires_grad_()\n",
    "            # 1. denoising step\n",
    "            x0_hat, eps_hat = denoiser(xt, t)\n",
    "            # 2. likelihood gradient approximation\n",
    "            likelihood_loss = get_likelihood_loss(x0_hat, y, physics).sum()\n",
    "        likelihood_grad = torch.autograd.grad(outputs=likelihood_loss, inputs=xt)[0].detach()\n",
    "        zeta_t = zeta / torch.linalg.vector_norm(likelihood_grad, ord=2, dim=(-2, -1), keepdim=True).detach()\n",
    "        \n",
    "        # 3. Denoising / Renoising step, determines x_{t-skip}' aka xt_next_prime\n",
    "        #  -> See Algorithm 1 (DPS - Gaussian) in DPS (Chung et al. 2022) paper\n",
    "        eps_next = torch.randn_like(xt)\n",
    "        sigma_tilde = torch.sqrt(b_t * (1 - abar_t_next) / (1 - abar_t))  # see Eq. 7 in DDPM paper\n",
    "\n",
    "        # reformulation in terms of x0hat, noise_estimate and z       \n",
    "        xt_next_prime = abar_t_next.sqrt() * x0_hat + (1 - abar_t_next - sigma_tilde**2).sqrt() * eps_hat \\\n",
    "            + sigma_tilde * eps_next\n",
    "\n",
    "        # 4. Measurement likelihood step, determines x_{t-skip} aka xt_next\n",
    "        #  -> We apply the step size scaling proposed in DPS,\n",
    "        #     see footnote 5 on page 6 of the DPS paper,\n",
    "        #     and then apply the gradient with the calculated step size.\n",
    "        xt_next = xt_next_prime - zeta_t * likelihood_grad\n",
    "\n",
    "        x0_preds_fast.append(x0_hat.to(\"cpu\"))\n",
    "        xs_fast.append(xt_next.to(\"cpu\"))\n",
    "\n",
    "recon = xs_fast[-1]\n",
    "\n",
    "# plot the results\n",
    "imgs = [y, recon, x_true]\n",
    "plot(imgs, titles=[\"measurement\", \"model output\", \"ground-truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_f, axs_f = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(7, 2.5))\n",
    "shown_measurement = y.detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1)\n",
    "shown_measurement[shown_measurement.isclose(torch.tensor(0.0))] = np.nan  # hide masked-out pixels (zeros)\n",
    "im0_f = axs_f[0].imshow((shown_measurement + 1) / 2)\n",
    "im1_f = axs_f[1].imshow((xs_fast[-1].detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)\n",
    "im2_f = axs_f[2].imshow((x_true.detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)\n",
    "axs_f[0].set_title('Measurement')\n",
    "axs_f[1].set_title('Model output')\n",
    "axs_f[2].set_title('Ground truth')\n",
    "for ax in axs_f:\n",
    "    ax.axis('off')\n",
    "fig_f.tight_layout()\n",
    "\n",
    "@widgets.interact(t=(0, num_steps-1, 1))\n",
    "def update(t=0):\n",
    "    im1_f.set_data((xs_fast[num_steps - t - 1].detach().cpu()[0].moveaxis(0, 2).clip(min=-1, max=1) + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
