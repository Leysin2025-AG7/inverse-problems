{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51842f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import numpy as np\n",
    "import ct_utils\n",
    "import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display\n",
    "import pywt\n",
    "\n",
    "\n",
    "def todo():\n",
    "    raise NotImplementedError(\"In dieser Zelle gibt es noch mindestens ein TODO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74423e4",
   "metadata": {},
   "source": [
    "# Teil II: Modellbasierte Regularisierung\n",
    "\n",
    "Eine beliebte Klasse von Regularisierungsmethoden lässt sich als Variationsproblem formulieren, bei dem die Rekonstruktion $x^*$ einer verrauschten Messung $y^{\\epsilon}$ als Lösung von\n",
    "$$ \\min_x \\frac{1}{2}\\|Ax-y^{\\varepsilon}\\|^2_2 + \\lambda J(x), $$\n",
    "gesucht wird.\n",
    "\n",
    "Die Funktion $J$ heißt Regularisierungsfunktion und bestraft unerwünschtes Verhalten von $x$. In diesem Teil des Tutorials werden wir uns typische Beispiele für $J$ ansehen und eine Vorstellung davon bekommen, wie der Minimierer des obigen Problems gefunden werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a3a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 128\n",
    "\n",
    "# Definiere eine ground truth (gt) - ein \"Phantom\"-Bild, siehe [https://de.wikipedia.org/wiki/Shepp-Logan-Phantom] (ctrl/cmd + click)\n",
    "x_gt = skimage.img_as_float(skimage.data.shepp_logan_phantom())\n",
    "x_gt = skimage.transform.resize(x_gt, (dim, dim))\n",
    "\n",
    "# Definiere den Vorwärtsoperator - bei CT ist das die Radontransformation\n",
    "theta = np.linspace(0, 180, endpoint=False, num=dim)\n",
    "A = ct_utils.Radon(theta)\n",
    "\n",
    "# Clean data\n",
    "y_clean = A(x_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4a730",
   "metadata": {},
   "source": [
    "## Tikhonov-Regularisierung\n",
    "\n",
    "Die erste Regularisierungsfunktion, die wir betrachten, ist $J(x) = \\frac{1}{2}\\|x\\|^2_2 $. Eine einfache Interpretation davon wäre, dass **Fehler klein gehalten werden, indem wir die Norm von x reduzieren**. Das Variationsproblem dafür lautet\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\varepsilon}\\|^2_2 + \\frac{\\lambda}{2} \\|x\\|^2_2.$$\n",
    "\n",
    "Analytisch lässt sich die Lösung des obigen Problems wäre $ x^* = (A^*A + \\lambda \\operatorname{Id})^{-1} A^*y^{\\varepsilon}$. Hier ist $A^*$ die Adjungierte von $A$ und $\\operatorname{Id}$ der Identitätsoperator.\n",
    "\n",
    "Da wir nicht einfach so auf die Inverse $(A^*A + \\lambda \\operatorname{Id})^{-1}$ zugreifen können, müssen wir als Ersatz eine iterative Optimierungsmethode wie z.B. gradient descent mit Schrittweite $\\eta > 0$ verwenden. Gradient descent ist ein iteratives Verfahren zum Lösen von Optimierungsproblemen der Form $\\min_x f(x)$: Wir starten bei einem Anfangsbild $x^0$ und berechnen iterativ für $k\\ge 1$ die Schritte\n",
    "$$ x^{k+1} = x^k - \\eta \\nabla f(x^k).$$\n",
    "In unserem speziellen Fall heißt das, wir iterieren\n",
    "$$ x^{k+1} = x^k - \\eta (A^*Ax^{k} + \\lambda x^k - A^*y^{\\varepsilon} ).\\tag{$\\ast$}$$\n",
    "Für $k \\to \\infty$ können wir (falls $\\eta$ richtig gewählt wird) erwarten, dass die Iterierten $x^k$ zur Lösung $x^*$ konvergieren. In der Praxis bricht man normalerweise nach einigen Schritten die Iteration ab und hofft, nahe genug an $x^*$ gelangt zu sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7cb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradient_descent(optimizers.optimizer):\n",
    "    def __init__(self, A, x, y, eta=0.1, lamda=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.A = A\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.eta = eta\n",
    "        self.lamda = lamda\n",
    "\n",
    "        def energy_fun(x):\n",
    "            return 0.5 * np.linalg.norm(A(x) - y) ** 2 + lamda * 0.5 * np.linalg.norm(\n",
    "                x, ord=1\n",
    "            )\n",
    "\n",
    "        self.energy_fun = energy_fun\n",
    "\n",
    "    def step(self):\n",
    "        # Implementiert hier den Gradient aus (*)\n",
    "        gradient = todo() # Hinweis: Die Adjungierte A^* von self.A ist implementiert als self.A.adjoint\n",
    "        # Hier sollte das Update\n",
    "        self.x = todo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe601b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5834459793a74b788e87e3639939220b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='lamda', max=0.5, readout_fo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_compute_tikh = {}\n",
    "\n",
    "\n",
    "def plot_tikh_reco(lamda, noise_lvl, max_angle):\n",
    "    num_theta = int(np.ceil(max_angle / 180 * dim))\n",
    "    theta = np.linspace(0, max_angle, endpoint=False, num=num_theta)\n",
    "    R = ct_utils.Radon(theta=theta)\n",
    "    if (lamda, noise_lvl, max_angle) in pre_compute_tikh:\n",
    "        x = pre_compute_tikh[(lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        y_noisy = R(x_gt) + np.random.normal(\n",
    "            loc=0, scale=noise_lvl, size=[dim, num_theta]\n",
    "        )\n",
    "\n",
    "        gd = gradient_descent(\n",
    "            R,\n",
    "            R.inv(y_noisy), # Wir starten bei einem educated guess x^0, der FBP von y_noisy\n",
    "            y_noisy,\n",
    "            eta=1 / (noise_lvl * lamda * 20000) if (noise_lvl * lamda > 0) else 0.0001,\n",
    "            lamda=lamda,\n",
    "            verbosity=0,\n",
    "            max_it=50,\n",
    "        )\n",
    "        gd.solve()\n",
    "\n",
    "        x = gd.x\n",
    "        pre_compute_tikh[(lamda, noise_lvl, max_angle)] = x\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "l_slider = widgets.FloatSlider(\n",
    "    min=0, max=0.5, step=0.01, value=0, continuous_update=False, readout_format=\".3f\"\n",
    ")\n",
    "s_slider = widgets.FloatSlider(\n",
    "    min=0.001,\n",
    "    max=0.01,\n",
    "    step=0.001,\n",
    "    value=0.001,\n",
    "    continuous_update=False,\n",
    "    readout_format=\".3f\",\n",
    ")\n",
    "t_slider = widgets.IntSlider(\n",
    "    min=1, max=180, step=10, value=180, continuous_update=False\n",
    ")\n",
    "interactive_plot = interactive(\n",
    "    plot_tikh_reco, lamda=l_slider, noise_lvl=s_slider, max_angle=t_slider\n",
    ")\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929cd8",
   "metadata": {},
   "source": [
    "## Sparsity-fördernde Regularisierung\n",
    "Eine weitere Anforderung an die Regularisierungen könnte sein, dass die erhaltenen Rekonstruktionen in gewisser Weise **einfach/aus wenigen Vektoren einer passenden Basis zusammensetzbar** sein sollten. \n",
    "\n",
    "Die beiden Komponenten, die wir benötigen, um dies zu erreichen, sind\n",
    "* ein Operator $D$, der $x$ in die Basis zerlegt, beispielsweise eine Wavelet-Zerlegung\n",
    "* die $\\|\\cdot\\|_1$-Norm, die die Sparsity fördert, d. h. Nicht-Null-Einträge von $Dx$ bestraft.\n",
    "\n",
    "Das resultierende Variationsproblem lautet dann\n",
    "$$ x^* = \\operatorname*{arg\\ min}_x \\frac{1}{2}\\|Ax-y^{\\varepsilon}\\|^2_2 + \\lambda \\|Dx\\|_1.$$\n",
    "\n",
    "Da die Regularisierungsfunktion im Allgemeinen nicht differenzierbar ist, verwenden wir statt gradient descent jetzt proximal gradient descent. Die proximale Abbildung einer Funktion $J$ mit dem Parameter $\\lambda$ ist definiert als\n",
    "$$ \\operatorname{prox}_{\\lambda J}(x) = \\operatorname*{arg\\ min}_z \\frac{1}{2}\\|x-z\\|_2^2 + \\lambda J(z),$$\n",
    "und ist im Spezialfall der $\\|\\cdot\\|_1$-Norm leicht berechenbar. Das Update von proximalem Gradientenabstieg lautet dann \n",
    "$$ x^{k+1} = \\operatorname{prox}_{t\\lambda J} (x^k - t \\cdot (A^*Ax^k - A^*y^\\varepsilon )).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d900b26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3dcb29282042808f9fff03f3c3d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Decomposing operator D', options=(('Identity', 0), ('Haar-Wav…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_compute_sparse = {}\n",
    "\n",
    "\n",
    "def plot_sparse_reco(D_idx, lamda, noise_lvl, max_angle):\n",
    "    num_theta = int(np.ceil(max_angle / 180 * dim))\n",
    "    theta = np.linspace(0, max_angle, endpoint=False, num=num_theta)\n",
    "    R = ct_utils.Radon(theta=theta)\n",
    "    if (D_idx, lamda, noise_lvl, max_angle) in pre_compute_sparse:\n",
    "        x = pre_compute_sparse[(D_idx, lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        sinogram = R(x_gt) + np.random.normal(\n",
    "            loc=0, scale=noise_lvl, size=[dim, num_theta]\n",
    "        )\n",
    "        opti = None\n",
    "        if D_idx == 0: # Dünnbesetztheit der Pixel der Lösung\n",
    "            opti = optimizers.ista_L1(\n",
    "                R,\n",
    "                R.inv(sinogram),\n",
    "                sinogram,\n",
    "                eta=1 / (noise_lvl * lamda * 100000)\n",
    "                if (noise_lvl * lamda > 0)\n",
    "                else 0.0001,\n",
    "                max_it=50,\n",
    "                lamda=lamda,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        elif D_idx == 1: # Dünnbesetztheit der Lösung in Haar-Wavelets\n",
    "            opti = optimizers.ista_wavelets(\n",
    "                R,\n",
    "                R.inv(sinogram),\n",
    "                sinogram,\n",
    "                wave=pywt.Wavelet(\"haar\"),\n",
    "                eta=1 / (noise_lvl * lamda * 100000)\n",
    "                if (noise_lvl * lamda > 0)\n",
    "                else 0.0001,\n",
    "                max_it=50,\n",
    "                lamda=lamda,\n",
    "                verbosity=0,\n",
    "            )\n",
    "        elif D_idx == 2: # Dünnbesetztheit der Lösung in Daubechies-Wavelets\n",
    "            opti = optimizers.ista_wavelets(\n",
    "                R,\n",
    "                R.inv(sinogram),\n",
    "                sinogram,\n",
    "                wave=pywt.Wavelet(\"db4\"),\n",
    "                eta=1 / (noise_lvl * lamda * 100000)\n",
    "                if (noise_lvl * lamda > 0)\n",
    "                else 0.0001,\n",
    "                max_it=50,\n",
    "                lamda=lamda,\n",
    "                verbosity=0,\n",
    "            )\n",
    "\n",
    "        opti.solve()\n",
    "\n",
    "        x = opti.x\n",
    "\n",
    "        pre_compute_sparse[(D_idx, lamda, noise_lvl, max_angle)] = x\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "idx_toggle = widgets.ToggleButtons(\n",
    "    options=[(\"Identity\", 0), (\"Haar-Wavelet\", 1), (\"Daubechies4-Wavelet\", 2)],\n",
    "    description=\"Decomposing operator D\",\n",
    "    disabled=False,\n",
    ")\n",
    "l_slider = widgets.FloatSlider(\n",
    "    min=0, max=0.2, step=0.004, value=0, continuous_update=False, readout_format=\".3f\"\n",
    ")\n",
    "s_slider = widgets.FloatSlider(\n",
    "    min=0.001,\n",
    "    max=0.01,\n",
    "    step=0.001,\n",
    "    value=0.001,\n",
    "    continuous_update=False,\n",
    "    readout_format=\".3f\",\n",
    ")\n",
    "t_slider = widgets.IntSlider(\n",
    "    min=1, max=180, step=10, value=180, continuous_update=False\n",
    ")\n",
    "interactive_plot = interactive(\n",
    "    plot_sparse_reco,\n",
    "    D_idx=idx_toggle,\n",
    "    lamda=l_slider,\n",
    "    noise_lvl=s_slider,\n",
    "    max_angle=t_slider,\n",
    ")\n",
    "\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f164f",
   "metadata": {},
   "source": [
    "# TV Regularisierung\n",
    "\n",
    "Aufgabe: Wie genau regularisiert TV die Lösung $x$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4021431d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2a8c7d392c4e7eb961e218ba86ddf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='lamda', max=0.01, readout_f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precompute_tv = {}\n",
    "\n",
    "\n",
    "def plot_tv_reco(lamda, noise_lvl, max_angle):\n",
    "    lamda, noise_lvl, max_angle = (round(lamda, 4), round(noise_lvl, 4), int(max_angle))\n",
    "\n",
    "    num_theta = int(np.ceil(max_angle / 180 * dim))\n",
    "    theta = np.linspace(0, max_angle, endpoint=False, num=num_theta)\n",
    "    R = ct_utils.Radon(theta=theta)\n",
    "    if (lamda, noise_lvl, max_angle) in precompute_tv:\n",
    "        x = pre_compute_sparse[(lamda, noise_lvl, max_angle)]\n",
    "    else:\n",
    "        y_noisy = R(x_gt) + np.random.normal(\n",
    "            loc=0, scale=noise_lvl, size=[dim, num_theta]\n",
    "        )\n",
    "\n",
    "        # tv = Total Variation\n",
    "        tv = optimizers.TV()\n",
    "\n",
    "        def energy_fun(x):\n",
    "            return np.linalg.norm(R(x) - y_noisy) ** 2 + lamda * tv(x)\n",
    "\n",
    "        sBTV = optimizers.split_Bregman_TV(\n",
    "            R,\n",
    "            y_noisy,\n",
    "            R.inv(y_noisy),\n",
    "            energy_fun=energy_fun,\n",
    "            lamda=lamda,\n",
    "            max_it=10,\n",
    "            max_inner_it=2,\n",
    "            verbosity=0,\n",
    "        )\n",
    "        sBTV.solve()\n",
    "\n",
    "        x = sBTV.x\n",
    "\n",
    "        precompute_tv[(lamda, noise_lvl, max_angle)] = x\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(x, vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "l_slider = widgets.FloatSlider(\n",
    "    min=0, max=0.01, step=0.001, value=0, continuous_update=False, readout_format=\".3f\"\n",
    ")\n",
    "s_slider = widgets.FloatSlider(\n",
    "    min=0.001,\n",
    "    max=0.01,\n",
    "    step=0.001,\n",
    "    value=0.0,\n",
    "    continuous_update=False,\n",
    "    readout_format=\".3f\",\n",
    ")\n",
    "t_slider = widgets.IntSlider(\n",
    "    min=1, max=180, step=10, value=180, continuous_update=False\n",
    ")\n",
    "interactive_plot = interactive(\n",
    "    plot_tv_reco, lamda=l_slider, noise_lvl=s_slider, max_angle=t_slider\n",
    ")\n",
    "\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac216d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akademie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
